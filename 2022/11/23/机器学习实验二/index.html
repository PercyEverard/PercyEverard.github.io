<!-- build time:Sat Aug 05 2023 22:09:14 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="fengyepiaosa" href="https://percyeverard.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="fengyepiaosa" href="https://percyeverard.github.io/atom.xml"><link rel="alternate" type="application/json" title="fengyepiaosa" href="https://percyeverard.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://percyeverard.github.io/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E4%BA%8C/"><title>机器学习实验二 - Homework - Mathine Learning | Yume Shoka = fengyepiaosa = 记录生活！</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">机器学习实验二</h1><div class="meta"><span class="item" title="创建时间：2022-11-23 19:07:05"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2022-11-23T19:07:05+08:00">2022-11-23</time></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Yume Shoka</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gicitcxhpij20zk0m8hdt.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gipesng5oej20zk0m87d4.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclfb3vzhj20zk0m8wny.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giph4baakhj20zk0m8h5q.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclx6phq6j20zk0m8e36.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclffsa1cj20zk0m811l.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Mathine-Learning/" itemprop="item" rel="index" title="分类于 Mathine Learning"><span itemprop="name">Mathine Learning</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Homework/" itemprop="item" rel="index" title="分类于 Homework"><span itemprop="name">Homework</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://percyeverard.github.io/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E4%BA%8C/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Percy Everard"><meta itemprop="description" content="记录生活！, welcome to wenfeng's blog!"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="fengyepiaosa"></span><div class="body md" itemprop="articleBody"><p>这是机器学习课程上的一个小项目作业，具体是通过多种回归模型预测房价...</p><p><span id="more"></span></p><h1 id="机器学习实验二使用多种回归模型预测房价"><a class="anchor" href="#机器学习实验二使用多种回归模型预测房价">#</a> 机器学习实验二，使用多种回归模型预测房价</h1><blockquote><p>这次实验基于上一次对房价数据的预处理，使用多种回归模型对处理的数据进行预测任务。建立的回归模型主要有 Lasso 回归、弹性网络回归、岭回归、梯度加强回归、随机森林回归、XGBoost 回归以及 LightGBM 回归七种模型。建立完回归模型完成任务，还需要对各个模型的性能进行评估与选择。并且考虑用网格搜索方法对表现最好的模型进行进一步的优化。</p></blockquote><h1 id="实验说明"><a class="anchor" href="#实验说明">#</a> 实验说明</h1><p>预测房价实验对 1460 条房价数据、80 个房屋的属性进行分析，建立回归模型，然后预测 1459 条新房屋数据的价格。</p><p>本实验是预测房价实验的第二部分，使用上个实验处理好的数据建立机器学习模型，并且对模型性能进行评估。</p><h1 id="实验准备"><a class="anchor" href="#实验准备">#</a> 实验准备</h1><h2 id="实验环境"><a class="anchor" href="#实验环境">#</a> 实验环境</h2><p><em>在桌面上新建一个目录，名字叫 house_price。本实验中所有的数据集和代码都放在该目录下</em></p><p><strong>Spyder</strong></p><h2 id="准备数据集"><a class="anchor" href="#准备数据集">#</a> 准备数据集</h2><p>本实验需要 2 个数据集，数据集的名字叫 train.csv 和 test.csv，分别是房价的训练数据和测试数据。将 train.csv 和 test.csv 复制到刚刚创建的 house_price 目录下。</p><h1 id="建立回归模型"><a class="anchor" href="#建立回归模型">#</a> 建立回归模型</h1><h2 id="lasso回归"><a class="anchor" href="#lasso回归">#</a> Lasso 回归</h2><pre><code>#Lasso Regression
from sklearn.linear_model import Lasso
lasso = Lasso(alpha = 0.0005, random_state = 1)
</code></pre><blockquote><p>lasso [læˈsuː] (捕马、套牛等用的) 套索；用套索套捕 (动物)</p></blockquote><h2 id="弹性网络回归"><a class="anchor" href="#弹性网络回归">#</a> 弹性网络回归</h2><pre><code># Elastic Net Regerssion
from sklearn.linear_model import ElasticNet
enet = ElasticNet(alpla = 0.0005, l1_ratio = .9, random_state = 3)
</code></pre><blockquote><p>elastic [ɪˈlæstɪk] 有弹力的灵活的，可改变的，可伸缩的；橡皮圈</p><p>ratio [ˈreɪʃiəʊ] 比率，比例</p></blockquote><h2 id="岭回归"><a class="anchor" href="#岭回归">#</a> 岭回归</h2><pre><code># Kernel Ridge Regression
from sklearn.kernel_ridge import KernelRidger
krr = KernelRidge(alpha = 0.6, kernel = &quot;polynomial&quot;, degree = 2, coef0 = 2.5)
</code></pre><blockquote><p>ridge [rɪdʒ] 山脊；使隆起，使形成脊状</p><p>polynomial [ˌpɒli'nəʊmiəl] [ˌpɑli'noʊmiəl] 多项式的；多项式</p><p>elapsed [ɪˈlæpst] (时间) 消逝，流逝 <strong>-&gt;</strong> elapse 的过去分词和过去式</p></blockquote><hr><p>Lasso 回归有时也叫做线性回归的 L1 正则化，和 Ridge 回归的主要区别就是在正则化项，Ridge 回归用的是 L2 正则化，而 Lasso 回归用的是 L1 正则化。而弹性回归是岭回归和套索回归的混合技术，它同时使用 L2 和 L1 正则化。当有多个相关的特征时，弹性网络是有用的。套索回归很可能随机选择其中一个，而弹性回归很可能都会选择。<em>套索回归和核岭回归说到底就是在损失函数上加了两个不同的正则化项罢了，而弹性网络就是两个正则化项都加上去。但是引入核岭回归却不是直接从 linear_model 直接引入，而是单独出 sklearn.kernel_ridge 进行引入，真是奇怪？？？</em></p><p>Lasso 回归使得一些系数变小，甚至还是一些绝对值较小的系数直接变为 0，因此特别适用于参数数目缩减与参数的选择，因而用来估计稀疏参数的线性模型。</p><p>个人观点：但是 Lasso 回归有一个很大的问题，导致我们需要把它单独拎出来讲，就是它的损失函数不是连续可导的。由于 L1 范数用的是绝对值之和，导致损失函数有不可导的点。也就是说，我们的最小二乘法，梯度下降法，牛顿法与拟牛顿法对它统统失效了。🙄🙄🙄</p><h2 id="梯度加强回归"><a class="anchor" href="#梯度加强回归">#</a> 梯度加强回归</h2><pre><code># Gradient Boosting Regression
from sklearn.ensemble import GradientBoostingRegressor
gboost = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05, max_depth = 4,max_features = &quot;sqrt&quot;, min_samples_leaf = 15, min_samples_split = 10,loss=&quot;huber&quot;,random_state = 5)
</code></pre><blockquote><p>ensemble [ɒnˈsɒmbl] n. 乐团，全体，成套的东西；adv. 一起</p><p>sklearn.ensemble 我理解成 Gradient Boosting Regression 用得比较广泛，导致它放在了 sklearn 的全体 kit 里面</p></blockquote><p>我尝试搜寻梯度加强回归，但是只能搜索到梯度提升回归树，看来它是一种树模型。* 它是一种决策树的集成方法，通过合并多个决策树来构建一个更为强大的模型。* 虽然他的名字中有回归两个字，但是这种模型既可以用在回归上，也可以用在分类上。</p><p>与随机森林不同的是，梯度提升采用连续的方式构造树，每一棵树都试图纠正前一棵树的错误。默认情况下，梯度提升回归树中没有随机化，而是用到了强预剪枝。</p><p>梯度提升决策树是监督学习中最强大也是最常用的模型之一，缺点是需要仔细调参，而且训练的时间要很长。和其他基于树的模型类似，这个算法不需要对数据进行缩放就可以表现的很好，并且它也适用于二元特征与连续特征同时存在的数据集。同样，它的不足之处与其它基于树的模型也类似，它不适合处理高维稀疏数据。</p><blockquote><p>所谓<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vSmV0cHJvcGVsbGVkU25ha2UvcC8xNDQzODQ4NC5odG1s">特征缩放</span>、<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzUxMjI4NTE1L2FydGljbGUvZGV0YWlscy8xMjI0MzE5NTE=">数据缩放</span>包含了归一化和标准化等。特征缩放的原因是，为消除各评价指标间量纲和数量级的差异、保证结果的可靠性，就需要对各指标的原始数据进行特征缩放。</p></blockquote><blockquote><p>个人观点：<em>对于稀疏数据，当前的研究方向是对稀疏数据进行聚类与降维。因为稀疏数据不同于一般数据，它的维度常常非常巨大，由于大量存在缺失值，导致数据信息极其不完整。常见的一些降维方法比如说主成分分析和因子分子方法都无法对稀疏数据进行应用</em></p></blockquote><h2 id="随机森林回归"><a class="anchor" href="#随机森林回归">#</a> 随机森林回归</h2><pre><code># random forest
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators = 500, random_state = 0)
</code></pre><p>随机森林属于<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpeXVhbmd1bHUvYXJ0aWNsZS9kZXRhaWxzLzEyMjUwODkzMT9zcG09MTAwMS4yMTAxLjMwMDEuNjY1MC44JmFtcDt1dG1fbWVkaXVtPWRpc3RyaWJ1dGUucGNfcmVsZXZhbnQubm9uZS10YXNrLWJsb2ctMiU3RWRlZmF1bHQlN0VCbG9nQ29tbWVuZEZyb21CYWlkdSU3RVJhdGUtOC0xMjI1MDg5MzEtYmxvZy04MjAxNjQ0Mi5wY19yZWxldmFudF9yZWNvdmVyeV92MiZhbXA7ZGVwdGhfMS11dG1fc291cmNlPWRpc3RyaWJ1dGUucGNfcmVsZXZhbnQubm9uZS10YXNrLWJsb2ctMiU3RWRlZmF1bHQlN0VCbG9nQ29tbWVuZEZyb21CYWlkdSU3RVJhdGUtOC0xMjI1MDg5MzEtYmxvZy04MjAxNjQ0Mi5wY19yZWxldmFudF9yZWNvdmVyeV92MiZhbXA7dXRtX3JlbGV2YW50X2luZGV4PTEx">集成学习</span>中的 Bagging 算法，即引导聚合类算法，详情可以点击上面链接进行访问。🪐</p><p>首先要明确的是，当前集成学习分成两大类，一类是 Bagging 装袋法，第二类是 Boosting 提升法。</p><ul><li>Bagging（装袋法）: 如随机森林，每一个模型相互独立，互相平行</li><li>Boosting（提升法）: 模型循序渐进，依次增强，基评估器相互关联</li></ul><p>我们都知道随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树。直观的说，每一棵决策树都是一个分类器（假设现在讨论的是分类问题），那么对于一个输入样本，N 棵树有 N 个分类结果。而随机森林做的就是集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。</p><p>在该模型中，有三个重要的参数分别为: &amp;nbsp <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzQ3OTk0Ny9hcnRpY2xlL2RldGFpbHMvMTI2ODEzMDMz">参考</span></p><ul><li>n_estimators (子树数量)</li><li>learning_rate (学习率)</li><li>max_depth (最大深度)</li></ul><p>想要进一步了解随机森林，可以通过以下链接进行学习：</p><blockquote><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpeXVhbmd1bHUvYXJ0aWNsZS9kZXRhaWxzLzEyMjUwODkzMT9zcG09MTAwMS4yMTAxLjMwMDEuNjY1MC44JmFtcDt1dG1fbWVkaXVtPWRpc3RyaWJ1dGUucGNfcmVsZXZhbnQubm9uZS10YXNrLWJsb2ctMiU3RWRlZmF1bHQlN0VCbG9nQ29tbWVuZEZyb21CYWlkdSU3RVJhdGUtOC0xMjI1MDg5MzEtYmxvZy04MjAxNjQ0Mi5wY19yZWxldmFudF9yZWNvdmVyeV92MiZhbXA7ZGVwdGhfMS11dG1fc291cmNlPWRpc3RyaWJ1dGUucGNfcmVsZXZhbnQubm9uZS10YXNrLWJsb2ctMiU3RWRlZmF1bHQlN0VCbG9nQ29tbWVuZEZyb21CYWlkdSU3RVJhdGUtOC0xMjI1MDg5MzEtYmxvZy04MjAxNjQ0Mi5wY19yZWxldmFudF9yZWNvdmVyeV92MiZhbXA7dXRtX3JlbGV2YW50X2luZGV4PTEx">CSDN</span></p><p><span class="exturl" data-url="aHR0cHM6Ly9lYXN5YWkudGVjaC9haS1kZWZpbml0aW9uL3JhbmRvbS1mb3Jlc3Qv">EasyAi</span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbWF5YmUyMDMwL3AvNDU4NTcwNS5odG1s">博客园</span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZmlvbmFjYWkvcC81ODk0MTQyLmh0bWw=">博客园</span></p></blockquote><h2 id="xgboost回归"><a class="anchor" href="#xgboost回归">#</a> XGBoost 回归</h2><pre><code># XGBoost
# if xgboost packet doesnt exist, please install with command:conda install py-xgboost
import xgboost as xbg
xgb = xgb.XGBRegressor(colsample_bytree = 0.4603, gamma = 0.0468,learning_rate = 0.05, max_depth = 3, min_child_weight = 1.7817, n_estimators = 2200, reg_alpha = 0.4640, reg_lambda = 0.8571,subsample = 0.5213, silent = 1, random_state = 7, nthread = -1)
</code></pre><p>首先要知道的是，XGBoost 算法是梯度提升树 GBDT 的高效实现。这里所提到的 GBDT 可能和上面所提到的梯度提升回归树有着异曲同工之处。作为梯度提升回归树的高效实现，XGBoost 是一个上限非常高的算法。在竞赛题中，不考虑深度学习，XGBoost 算法算是比赛中最热门的算法，它将 GBDT 的优化走向了一个极致。但是它的训练耗时长，内存占用比较大。</p><p>后续，微软推出了 LightGBM，在内存占用和运行速度上做了不少的优化，但是<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvMTA5Nzk4MDguaHRtbA==">刘建平 pinard 大哥</span>认为当前还是优先选择 XGBoost，因为调优经验比较多一点，可以参考的资料也更多一些。如果使用 XGBoost 遇到的内存占用或者运行速度问题，这个时候再尝试 LightGBM 也不错。</p><h2 id="lightgbm回归"><a class="anchor" href="#lightgbm回归">#</a> LightGBM 回归</h2><pre><code># LightGBM
import lightgbm as lgb
lgb = lgb.LGBMRegressor(objective = &quot;regression&quot;, num_leaves = 5, learning_rate = 0.05, n_estimators = 720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed = 9, bagging_seed = 9, min_data_in_leaf = 6,min_sum_hessian_in_leaf = 11)
</code></pre><blockquote><p>fraction [ˈfrækʃn] n. 小部分，分数，小数</p></blockquote><p>LightGBM 是快速的，分布式的，高性能的基于决策树算法的梯度提升框架。可以用于排序、分类、回归以及其他机器学习任务中。它是 XGBoost 的改进版，XGboost 又是 GBDT 梯度提升回归树的高效实现，所以我理解成 LightGBM 与 GBDT 师出同门。</p><p>2017 年的时候微软在 GitHub 上开源了一个新的算法<strong> LightGBM</strong>，根据某 CSDN 机翻文章称，它能够在不降低准确率的前提下，能够提升十倍左右的速度，并且占用内存下降三倍左右。虽然它和 XGboost 都是基于决策树算法的，但是 LightGBM 采用的分裂叶子结点策略貌似与 XGBoost 算法不同，并且极大地优于 XGBoost，导致它的速度非常之快...</p><p><strong>到目前为止，七个回归模型的初始化都已经完成！</strong></p><h1 id="模型性能评估与选择"><a class="anchor" href="#模型性能评估与选择">#</a> 模型性能评估与选择</h1><p>业界一般使用 k-fold 交叉验证（k-fold cross validation）的方法评估模型性能。在这里 k=10。评估指标有 2 个，一个是 r^2（ r squared）, 另一个是 mse（mean squared error）。为了加快执行速度，n_jobs = 2 表示使用 2 个 CPU（我的电脑是 2 核 CPU）。训练数据集有 1458 条数据，272 列，模型执行起来可能相对较慢，请耐心等待。verbose = 1 表示执行过程中不断打印出信息，以帮助判断执行了多少了。</p><p><strong>提示</strong>：同学们可能疑惑为什么没有在验证集上检验模型性能，这可能会产生过拟合现象。k-fold 交叉验证的存在可以在不需要验证集的情况下，也能发现过拟合现象😅。</p><pre><code># k-fold cross validation function
from sklearn.model_selection import cross_val_score
def evaluation_model(model):
    rmse = np.sqrt(-cross_val_score(estimator = model, X = X_train, y = y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = 2, verbose = 1))
    r2_score = cross_val_score(estimator = model, X = X_train, y = y_train, scoring = 'r2', cv = 10, n_jobs = 2, verbose = 1)
    return (r2_score, rmse)
# print result function
def print_result(r2_score, rmse, model_name):
    print('%s evaluation: r2=%.4f (std=%.4f), rmse=%.4f (std=%.4f)' %(model_name, r2_score.mean(), r2_score.std(), 
  rmse.mean(), rmse.std()))
</code></pre><p>定义好了函数，现在可以对每一模型执行。</p><pre><code># model score and rmse
lasso_r2_score, lasso_rmse = evaluation_model(lasso)
enet_r2_score, enet_rmse = evaluation_model(enet)
krr_r2_score, krr_rmse = evaluation_model(krr)
gboost_r2_score, gboost_rmse = evaluation_model(gboost)
rf_r2_score, rf_rmse = evaluation_model(rf)
xgb_r2_score, xgb_rmse = evaluation_model(xgb)
lgb_r2_score, lgb_rmse = evaluation_model(lgb)
# print result
print_result(lasso_r2_score, lasso_rmse, 'Lasso')
print_result(enet_r2_score, enet_rmse, 'Elastic Net')
print_result(krr_r2_score, krr_rmse, 'Kernel Ridge')
print_result(gboost_r2_score, gboost_rmse, 'Gradient Boost')
print_result(rf_r2_score, rf_rmse, 'Random Forest')
print_result(xgb_r2_score, xgb_rmse, 'XG Boost')
print_result(lgb_r2_score, lgb_rmse, 'Lightgbm')
</code></pre><p>模型性能的评估尤其是用 sklearn 进行模型评估貌似篇幅有点小长，我改天再写吧！现在可以看看别人对模型评估方法进行的一些<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vamlheGluMzU5L3AvODYyNzUzMC5odG1s">总结</span>，以及<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTc1NzcwNC9hcnRpY2xlL2RldGFpbHMvMTE4NDA2MjU1">运用</span>。</p><h1 id="运行结果"><a class="anchor" href="#运行结果">#</a> 运行结果</h1><p>结果发现 XGBoost 的性能最好:r2=0.9227 (std=0.0149), rmse=0.2734 (std=0.0340)，因为 RMSE 平均数最小，方差最小，并且 R2&gt;0.9，模型性能比较好。值得注意的是，如果你们选择的是模型默认的超参数，那么执行的结果和我的就不一样啦，因为不同的超参数决定了模型的准确率的不同。这里给出了一个通用的原则：</p><blockquote><p><strong>选择 r2&gt;0.9，且 mse 和 std 最小的那个模型</strong></p></blockquote><h1 id="使用gridsearchcv方法对xgboost模型进行优化"><a class="anchor" href="#使用gridsearchcv方法对xgboost模型进行优化">#</a> 使用 GridSearchCV 方法对 XGBoost 模型进行优化</h1><p>在机器学习模型中，需要人工选择的参数称为超参数。比如随机森林中决策树的个数，人工神经网络模型中隐藏层层数和每层的节点个数，正则项中常数大小等等，他们都需要事先指定。超参数选择不恰当，就会出现欠拟合或者过拟合的问题。而在选择超参数的时候，有两个途径，一个是凭经验微调，另一个就是选择不同大小的参数，带入模型中，挑选表现最好的参数。</p><p>微调的一种方法是手工调制超参数，直到找到一个好的超参数组合，这么做的话会非常冗长，你也可能没有时间探索多种组合，所以可以使用 Scikit-Learn 的 GridSearchCV 来做这项搜索工作。</p><p>这里可以将 Grid Search 暂时理解为是一种调参手段，并且是非常耗时的穷举搜索。* 至于进一步深究，我们暂时放到下一步。因为今天的篇幅有点长，担心如果我再展开讲下去的话会增加各位读者包括我的阅读负担。所以我打算把网格搜索放到了下一篇（也可能是下下篇）* 你可以先看看<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3ljNDM5L2FydGljbGUvZGV0YWlscy8xMjMzODE5MDg=">这里</span>🍉🍉🍉</p><h1 id="参考链接"><a class="anchor" href="#参考链接">#</a> 参考链接</h1><blockquote><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9fL2FydGljbGUvZGV0YWlscy84MTA1NzE1MA==">https://blog.csdn.net/huacha__/article/details/81057150</span></p><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZlbmdodWliaWFuL2FydGljbGUvZGV0YWlscy85MTM1MzM0OA==">https://blog.csdn.net/fenghuibian/article/details/91353348</span></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC8xNzM2ODk4OGQ2ZDk=">监督学习 (八)—— 决策树集成：梯度提升回归树</span></p><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzQ3OTk0Ny9hcnRpY2xlL2RldGFpbHMvMTI2ODEzMDMz">也是梯度提升回归树</span></p></blockquote></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2022-11-23 19:45:04" itemprop="dateModified" datetime="2022-11-23T19:45:04+08:00">2022-11-23</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Percy Everard 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="Percy Everard 支付宝"><p>支付宝</p></div><div><img data-src="/images/paypal.png" alt="Percy Everard 贝宝"><p>贝宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Percy Everard <i class="ic i-at"><em>@</em></i>fengyepiaosa</li><li class="link"><strong>本文链接：</strong> <a href="https://percyeverard.github.io/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E4%BA%8C/" title="机器学习实验二">https://percyeverard.github.io/2022/11/23/机器学习实验二/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2022/11/23/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipesrnqv3j20zk0m8ava.jpg" title="激活函数"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> Mathine Learning</span><h3>激活函数</h3></a></div><div class="item right"><a href="/2022/11/23/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva1.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gicitf0kl1j20zk0m87fe.jpg" title="维度灾难"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> Mathine Learning</span><h3>维度灾难</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E4%BA%8C%E4%BD%BF%E7%94%A8%E5%A4%9A%E7%A7%8D%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7"><span class="toc-number">1.</span> <span class="toc-text">机器学习实验二，使用多种回归模型预测房价</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AF%B4%E6%98%8E"><span class="toc-number">2.</span> <span class="toc-text">实验说明</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%87%86%E5%A4%87"><span class="toc-number">3.</span> <span class="toc-text">实验准备</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83"><span class="toc-number">3.1.</span> <span class="toc-text">实验环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.2.</span> <span class="toc-text">准备数据集</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">建立回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#lasso%E5%9B%9E%E5%BD%92"><span class="toc-number">4.1.</span> <span class="toc-text">Lasso 回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%E5%9B%9E%E5%BD%92"><span class="toc-number">4.2.</span> <span class="toc-text">弹性网络回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">4.3.</span> <span class="toc-text">岭回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%8A%A0%E5%BC%BA%E5%9B%9E%E5%BD%92"><span class="toc-number">4.4.</span> <span class="toc-text">梯度加强回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92"><span class="toc-number">4.5.</span> <span class="toc-text">随机森林回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#xgboost%E5%9B%9E%E5%BD%92"><span class="toc-number">4.6.</span> <span class="toc-text">XGBoost 回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lightgbm%E5%9B%9E%E5%BD%92"><span class="toc-number">4.7.</span> <span class="toc-text">LightGBM 回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="toc-number">5.</span> <span class="toc-text">模型性能评估与选择</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">6.</span> <span class="toc-text">运行结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8gridsearchcv%E6%96%B9%E6%B3%95%E5%AF%B9xgboost%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E4%BC%98%E5%8C%96"><span class="toc-number">7.</span> <span class="toc-text">使用 GridSearchCV 方法对 XGBoost 模型进行优化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">8.</span> <span class="toc-text">参考链接</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2022/11/23/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B9%8B%E6%89%87%E5%BD%A2%E4%B8%8E%E7%9F%A9%E5%BD%A2%E5%9B%BE%E5%83%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2/" rel="bookmark" title="图像处理之扇形与矩形图像之间的相互转换">图像处理之扇形与矩形图像之间的相互转换</a></li><li class="active"><a href="/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E4%BA%8C/" rel="bookmark" title="机器学习实验二">机器学习实验二</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Percy Everard" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Percy Everard</p><div class="description" itemprop="description">welcome to wenfeng's blog!</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">68</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">25</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">55</span> <span class="name">标签</span></a></div></nav><div class="social"></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2022/11/23/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2022/11/23/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E9%87%91%E6%9B%B2%E5%85%B1%E8%B5%8F/" title="分类于 金曲共赏">金曲共赏</a></div><span><a href="/2023/05/08/Yawarakana-Hikari/" title="Yawarakana Hikari">Yawarakana Hikari</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E9%87%91%E6%9B%B2%E5%85%B1%E8%B5%8F/" title="分类于 金曲共赏">金曲共赏</a></div><span><a href="/2023/07/31/%E7%9B%B8%E7%88%B1%E5%BE%88%E9%9A%BE/" title="相爱很难">相爱很难</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2023/03/12/Ubuntu-practise-C-inward/" title="Ubuntu practise C++ inward">Ubuntu practise C++ inward</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Blog/" title="分类于 Blog">Blog</a></div><span><a href="/2023/01/13/Hexo%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4/" title="The commands i often use in HEXO">The commands i often use in HEXO</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Mathine-Learning/" title="分类于 Mathine Learning">Mathine Learning</a> <i class="ic i-angle-right"></i> <a href="/categories/Homework/" title="分类于 Homework">Homework</a></div><span><a href="/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E4%BA%8C/" title="机器学习实验二">机器学习实验二</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/blog-%E6%96%87%E7%AB%A0%E5%88%86%E7%B1%BB/" title="分类于 blog            //文章分类">blog //文章分类</a></div><span><a href="/2023/08/05/hello-world/" title="Hello World">Hello World</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Study/" title="分类于 Study">Study</a></div><span><a href="/2023/03/23/libGL/" title="Linux环境下Qt报错libGL">Linux环境下Qt报错libGL</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Transshipment/" title="分类于 Transshipment">Transshipment</a></div><span><a href="/2023/03/28/cplusplus-mutable-keyword/" title="cplusplus中的mutable keyword">cplusplus中的mutable keyword</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%9F%A5%E8%AF%86%E5%A4%A7%E9%99%86/" title="分类于 知识大陆">知识大陆</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%BC%96%E7%A8%8B%E6%8B%93%E5%B1%95/" title="分类于 编程拓展">编程拓展</a></div><span><a href="/2023/07/26/c-c-%E5%BC%BA%E5%88%B6%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/" title="c&#x2F;c++强制类型转换">c/c++强制类型转换</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/git-operation/" title="分类于 git operation">git operation</a></div><span><a href="/2022/11/20/github%E6%96%87%E4%BB%B6%E5%A4%B9%E6%9C%89%E7%99%BD%E8%89%B2%E7%AE%AD%E5%A4%B4%E5%B9%B6%E4%B8%94%E4%B8%8D%E8%83%BD%E6%89%93%E5%BC%80%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/" title="github文件夹有白色箭头并且不能打开的解决办法">github文件夹有白色箭头并且不能打开的解决办法</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Percy Everard @ Yume Shoka</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2022/11/23/机器学习实验二/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->