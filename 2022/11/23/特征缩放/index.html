<!-- build time:Mon Aug 07 2023 12:04:13 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="fengyepiaosa" href="https://percyeverard.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="fengyepiaosa" href="https://percyeverard.github.io/atom.xml"><link rel="alternate" type="application/json" title="fengyepiaosa" href="https://percyeverard.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://percyeverard.github.io/2022/11/23/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/"><title>特征缩放 - 知识大陆 | Percy Feng = fengyepiaosa = Neuromancer</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">特征缩放</h1><div class="meta"><span class="item" title="创建时间：2022-11-23 19:19:59"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2022-11-23T19:19:59+08:00">2022-11-23</time></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Percy Feng</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><img src="https://s2.loli.net/2023/08/06/zfA4ckxtN6KYX8T.jpg"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E7%9F%A5%E8%AF%86%E5%A4%A7%E9%99%86/" itemprop="item" rel="index" title="分类于 知识大陆"><span itemprop="name">知识大陆</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://percyeverard.github.io/2022/11/23/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/magic.png"><meta itemprop="name" content="Percy Everard"><meta itemprop="description" content="Neuromancer, Really start to record my life, no longer lazy!"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="fengyepiaosa"></span><div class="body md" itemprop="articleBody"><p>将数据进行归一化和标准化可以使不同维度的特征放在一起进行比较，可以大大提高模型的准确性。</p><p><span id="more"></span></p><h1 id="特征缩放"><a class="anchor" href="#特征缩放">#</a> 特征缩放</h1><h2 id="1-为什么要进行特征缩放"><a class="anchor" href="#1-为什么要进行特征缩放">#</a> 1. 为什么要进行特征缩放</h2><h3 id="11-统一特征的权重和提升模型准确性"><a class="anchor" href="#11-统一特征的权重和提升模型准确性">#</a> 1.1 统一特征的权重和提升模型准确性</h3><p>如果某个特征的取值范围比其他特征大很多，那么数值计算（比如说计算欧式距离）就受该特征的主要影响。但实际上并不一定是这个特征最重要，通常需要把每个特征看成同等重要。归一化和标准化数据可以使不同维度的特征放在一起进行比较，可以大大提高模型的准确性。</p><h3 id="12-提升梯度下降法的收敛速度"><a class="anchor" href="#12-提升梯度下降法的收敛速度">#</a> 1.2 提升梯度下降法的收敛速度</h3><p><img data-src="https://img-blog.csdnimg.cn/20191113192133110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzAwODgwNA==,size_16,color_FFFFFF,t_70" alt="左边是未归一化数据的梯度下降过程"></p><h2 id="2-特征缩放的方法有哪些"><a class="anchor" href="#2-特征缩放的方法有哪些">#</a> 2. 特征缩放的方法有哪些？</h2><h3 id="21-最大最小值归一化min-max-normalization将数值范围缩放到-0-1-区间里"><a class="anchor" href="#21-最大最小值归一化min-max-normalization将数值范围缩放到-0-1-区间里">#</a> 2.1 最大最小值归一化（min-max normalization）：将数值范围缩放到 [0, 1] 区间里</h3><p><img data-src="https://img2018.cnblogs.com/blog/1247570/201901/1247570-20190103162752557-711062993.png" alt=""></p><h3 id="22-均值归一化mean-normalization将数值范围缩放到-1-1-区间里且数据的均值变为0"><a class="anchor" href="#22-均值归一化mean-normalization将数值范围缩放到-1-1-区间里且数据的均值变为0">#</a> 2.2 均值归一化（mean normalization）：将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为 0</h3><p><img data-src="https://img2018.cnblogs.com/blog/1247570/201901/1247570-20190103162807303-1305019357.png" alt=""></p><h3 id="23-标准化-z-值归一化standardization-z-score-normalization将数值缩放到0附近且数据的分布变为均值为0标准差为1的标准正态分布先减去均值来对特征进行-中心化-mean-centering-处理再除以标准差进行缩放"><a class="anchor" href="#23-标准化-z-值归一化standardization-z-score-normalization将数值缩放到0附近且数据的分布变为均值为0标准差为1的标准正态分布先减去均值来对特征进行-中心化-mean-centering-处理再除以标准差进行缩放">#</a> 2.3 标准化，Z 值归一化（standardization /z-score normalization）：将数值缩放到 0 附近，且数据的分布变为均值为 0，标准差为 1 的标准正态分布（先减去均值来对特征进行 中心化 mean centering 处理，再除以标准差进行缩放）</h3><p><img data-src="https://img2018.cnblogs.com/blog/1247570/201901/1247570-20190103162903434-1406801774.png" alt=""></p><h3 id="24-最大绝对值归一化max-abs-normalization-也就是将数值变为单位长度scaling-to-unit-length将数值范围缩放到-1-1-区间里"><a class="anchor" href="#24-最大绝对值归一化max-abs-normalization-也就是将数值变为单位长度scaling-to-unit-length将数值范围缩放到-1-1-区间里">#</a> 2.4 最大绝对值归一化（max abs normalization ）：也就是将数值变为单位长度（scaling to unit length），将数值范围缩放到 [-1, 1] 区间里</h3><p><img data-src="https://img2018.cnblogs.com/blog/1247570/201901/1247570-20190112114759202-190163995.png" alt=""></p><h3 id="25-稳键标准化robust-standardization先减去中位数再除以四分位间距interquartile-range因为不涉及极值因此在数据里有异常值的情况下表现比较稳健"><a class="anchor" href="#25-稳键标准化robust-standardization先减去中位数再除以四分位间距interquartile-range因为不涉及极值因此在数据里有异常值的情况下表现比较稳健">#</a> 2.5 稳键标准化（robust standardization）：先减去中位数，再除以四分位间距（interquartile range），因为不涉及极值，因此在数据里有异常值的情况下表现比较稳健</h3><p><img data-src="https://img2018.cnblogs.com/blog/1247570/201908/1247570-20190811172514046-446486849.png" alt=""></p><blockquote><p><em>有一些时候，只对数据进行中心化和缩放是不够的，还需对数据进行白化（whitening）处理来消除特征间的线性相关性</em></p></blockquote><h2 id="3-标准化和归一化的区别是什么"><a class="anchor" href="#3-标准化和归一化的区别是什么">#</a> 3. 标准化和归一化的区别是什么？</h2><h3 id="31-归一化normalizationnormalization将一列数据变化到某个固定区间范围中通常这个区间是0-1广义的讲可以是各种区间比如映射到01一样可以继续映射到其他范围图像中可能会映射到0255其他情况可能映射到-11"><a class="anchor" href="#31-归一化normalizationnormalization将一列数据变化到某个固定区间范围中通常这个区间是0-1广义的讲可以是各种区间比如映射到01一样可以继续映射到其他范围图像中可能会映射到0255其他情况可能映射到-11">#</a> 3.1 归一化 (NormalizationNormalization)：将一列数据变化到某个固定区间 (范围) 中，通常，这个区间是 [0, 1]，广义的讲，可以是各种区间，比如映射到 [0，1] 一样可以继续映射到其他范围，图像中可能会映射到 [0,255]，其他情况可能映射到 [-1,1]</h3><h3 id="32-标准化standardizationstandardization将数据变换为均值为0标准差为1的分布切记并非一定是正态的"><a class="anchor" href="#32-标准化standardizationstandardization将数据变换为均值为0标准差为1的分布切记并非一定是正态的">#</a> 3.2 标准化 (StandardizationStandardization)：将数据变换为均值为 0，标准差为 1 的分布切记，并非一定是正态的</h3><p><img data-src="https://img-blog.csdnimg.cn/20191021220509274.png#pic_center" alt=""></p><h3 id="33-中心化另外还有一种处理叫做中心化也叫零均值处理就是将每个原始数据减去这些数据的均值"><a class="anchor" href="#33-中心化另外还有一种处理叫做中心化也叫零均值处理就是将每个原始数据减去这些数据的均值">#</a> 3.3 中心化：另外，还有一种处理叫做中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值</h3><p><img data-src="https://img-blog.csdnimg.cn/20191021220535129.png#pic_center" alt=""></p><h2 id="4-什么时候用标准化什么时候用归一化"><a class="anchor" href="#4-什么时候用标准化什么时候用归一化">#</a> 4. 什么时候用标准化，什么时候用归一化？</h2><p>某博主理解：如果你对处理后的数据范围有严格要求，那肯定是归一化，个人经验，标准化是机器学习中更通用的手段，如果你无从下手，可以直接使用标准化。如果数据不为稳定，存在极端的最大最小值，不要用归一化。</p><p>在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用 PCA 技术进行降维的时候，标准化表现更好；在不涉及距离度量、协方差计算的时候，可以使用归一化方法。</p><p><strong>在需要使用距离来度量相似性的算法中，或者使用 PCA 技术进行降维的时候，通常使用标准化（standardization）或均值归一化（mean normalization）比较好，但如果数据分布不是正态分布或者标准差非常小，以及需要把数据固定在 [0, 1] 范围内，那么使用最大最小值归一化（min-max normalization）比较好（min-max 常用于归一化图像的灰度值）。但是 min-max 比较容易受异常值的影响，如果数据集包含较多的异常值，可以考虑使用稳键归一化（robust normalization）。对于已经中心化的数据或稀疏数据的缩放，比较推荐使用最大绝对值归一化（max abs normalization ），因为它会保住数据中的０元素，不会破坏数据的稀疏性（sparsity）。</strong></p><h2 id="5-什么机器学习模型需要用到特征缩放"><a class="anchor" href="#5-什么机器学习模型需要用到特征缩放">#</a> 5. 什么机器学习模型需要用到特征缩放？</h2><h3 id="51-通过梯度下降法求解的模型需要进行特征缩放这包括线性回归linear-regression-逻辑回归logistic-regression-感知机perceptron-支持向量机svm-神经网络neural-network等模型-此外近邻法knnk均值聚类k-means等需要根据数据间的距离来划分数据的算法也需要进行特征缩放-主成分分析pca线性判别分析lda等需要计算特征的方差的算法也会受到特征缩放的影响"><a class="anchor" href="#51-通过梯度下降法求解的模型需要进行特征缩放这包括线性回归linear-regression-逻辑回归logistic-regression-感知机perceptron-支持向量机svm-神经网络neural-network等模型-此外近邻法knnk均值聚类k-means等需要根据数据间的距离来划分数据的算法也需要进行特征缩放-主成分分析pca线性判别分析lda等需要计算特征的方差的算法也会受到特征缩放的影响">#</a> 5.1 通过梯度下降法求解的模型需要进行特征缩放，这包括线性回归（Linear Regression）、逻辑回归（Logistic Regression）、感知机（Perceptron）、支持向量机（SVM）、神经网络（Neural Network）等模型。此外，近邻法（KNN），K 均值聚类（K-Means）等需要根据数据间的距离来划分数据的算法也需要进行特征缩放。主成分分析（PCA），线性判别分析（LDA）等需要计算特征的方差的算法也会受到特征缩放的影响。</h3><h3 id="52-决策树decision-tree随机森林random-forest等基于树的分类模型不需要进行特征缩放因为特征缩放不会改变样本在特征上的信息增益"><a class="anchor" href="#52-决策树decision-tree随机森林random-forest等基于树的分类模型不需要进行特征缩放因为特征缩放不会改变样本在特征上的信息增益">#</a> 5.2 决策树（Decision Tree），随机森林（Random Forest）等基于树的分类模型不需要进行特征缩放，因为特征缩放不会改变样本在特征上的信息增益。</h3><h2 id="6-在进行特征缩放的时候需要注意的地方"><a class="anchor" href="#6-在进行特征缩放的时候需要注意的地方">#</a> 6. 在进行特征缩放的时候需要注意的地方？</h2><p><em>需要先把数据先拆分成训练集与验证集，在训练集上计算出需要的数值（如均值和标准值），对训练集数据做标准化 / 归一化处理，然后再用之前计算出的数据（如均值和标准值）对验证集数据做相同的标准化 / 归一化处理。不要在整个数据集上直接做标准化 / 归一化处理，因为这样会将验证集的信息带入到训练集中，这是一个非常容易犯的错误</em></p><h2 id="7-参考链接"><a class="anchor" href="#7-参考链接">#</a> 7. 参考链接</h2><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vSHVaaWh1L3AvOTc2MTE2MS5odG1s">https://www.cnblogs.com/HuZihu/p/9761161.html</span></p><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzAwODgwNC9hcnRpY2xlL2RldGFpbHMvMTAzMDg3NDQ3">https://blog.csdn.net/weixin_43008804/article/details/103087447</span></p><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjYwNDk1My9hcnRpY2xlL2RldGFpbHMvMTAyNjUyMTYw">https://blog.csdn.net/weixin_36604953/article/details/102652160</span></p></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2023-08-06 14:58:46" itemprop="dateModified" datetime="2023-08-06T14:58:46+08:00">2023-08-06</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Percy Everard <i class="ic i-at"><em>@</em></i>fengyepiaosa</li><li class="link"><strong>本文链接：</strong> <a href="https://percyeverard.github.io/2022/11/23/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/" title="特征缩放">https://percyeverard.github.io/2022/11/23/特征缩放/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2022/11/23/Jupyter-Notebook/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2023&#x2F;08&#x2F;06&#x2F;KJYI63A8jL51cOC.jpg" title="Jupyter Notebook"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 知识大陆</span><h3>Jupyter Notebook</h3></a></div><div class="item right"><a href="/2022/11/23/L1%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2023&#x2F;08&#x2F;06&#x2F;CqpbejH5U7KEyxi.jpg" title="L1和L2正则化"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 知识大陆</span><h3>L1和L2正则化</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">1.</span> <span class="toc-text">特征缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">1.1.</span> <span class="toc-text">1. 为什么要进行特征缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E7%BB%9F%E4%B8%80%E7%89%B9%E5%BE%81%E7%9A%84%E6%9D%83%E9%87%8D%E5%92%8C%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E5%87%86%E7%A1%AE%E6%80%A7"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 统一特征的权重和提升模型准确性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E6%8F%90%E5%8D%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 提升梯度下降法的收敛速度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E7%9A%84%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">1.2.</span> <span class="toc-text">2. 特征缩放的方法有哪些？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96min-max-normalization%E5%B0%86%E6%95%B0%E5%80%BC%E8%8C%83%E5%9B%B4%E7%BC%A9%E6%94%BE%E5%88%B0-0-1-%E5%8C%BA%E9%97%B4%E9%87%8C"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 最大最小值归一化（min-max normalization）：将数值范围缩放到 [0, 1] 区间里</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96mean-normalization%E5%B0%86%E6%95%B0%E5%80%BC%E8%8C%83%E5%9B%B4%E7%BC%A9%E6%94%BE%E5%88%B0-1-1-%E5%8C%BA%E9%97%B4%E9%87%8C%E4%B8%94%E6%95%B0%E6%8D%AE%E7%9A%84%E5%9D%87%E5%80%BC%E5%8F%98%E4%B8%BA0"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 均值归一化（mean normalization）：将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为 0</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#23-%E6%A0%87%E5%87%86%E5%8C%96-z-%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96standardization-z-score-normalization%E5%B0%86%E6%95%B0%E5%80%BC%E7%BC%A9%E6%94%BE%E5%88%B00%E9%99%84%E8%BF%91%E4%B8%94%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E5%B8%83%E5%8F%98%E4%B8%BA%E5%9D%87%E5%80%BC%E4%B8%BA0%E6%A0%87%E5%87%86%E5%B7%AE%E4%B8%BA1%E7%9A%84%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E5%85%88%E5%87%8F%E5%8E%BB%E5%9D%87%E5%80%BC%E6%9D%A5%E5%AF%B9%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C-%E4%B8%AD%E5%BF%83%E5%8C%96-mean-centering-%E5%A4%84%E7%90%86%E5%86%8D%E9%99%A4%E4%BB%A5%E6%A0%87%E5%87%86%E5%B7%AE%E8%BF%9B%E8%A1%8C%E7%BC%A9%E6%94%BE"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 标准化，Z 值归一化（standardization &#x2F;z-score normalization）：将数值缩放到 0 附近，且数据的分布变为均值为 0，标准差为 1 的标准正态分布（先减去均值来对特征进行 中心化 mean centering 处理，再除以标准差进行缩放）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#24-%E6%9C%80%E5%A4%A7%E7%BB%9D%E5%AF%B9%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96max-abs-normalization-%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%B0%86%E6%95%B0%E5%80%BC%E5%8F%98%E4%B8%BA%E5%8D%95%E4%BD%8D%E9%95%BF%E5%BA%A6scaling-to-unit-length%E5%B0%86%E6%95%B0%E5%80%BC%E8%8C%83%E5%9B%B4%E7%BC%A9%E6%94%BE%E5%88%B0-1-1-%E5%8C%BA%E9%97%B4%E9%87%8C"><span class="toc-number">1.2.4.</span> <span class="toc-text">2.4 最大绝对值归一化（max abs normalization ）：也就是将数值变为单位长度（scaling to unit length），将数值范围缩放到 [-1, 1] 区间里</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#25-%E7%A8%B3%E9%94%AE%E6%A0%87%E5%87%86%E5%8C%96robust-standardization%E5%85%88%E5%87%8F%E5%8E%BB%E4%B8%AD%E4%BD%8D%E6%95%B0%E5%86%8D%E9%99%A4%E4%BB%A5%E5%9B%9B%E5%88%86%E4%BD%8D%E9%97%B4%E8%B7%9Dinterquartile-range%E5%9B%A0%E4%B8%BA%E4%B8%8D%E6%B6%89%E5%8F%8A%E6%9E%81%E5%80%BC%E5%9B%A0%E6%AD%A4%E5%9C%A8%E6%95%B0%E6%8D%AE%E9%87%8C%E6%9C%89%E5%BC%82%E5%B8%B8%E5%80%BC%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E8%A1%A8%E7%8E%B0%E6%AF%94%E8%BE%83%E7%A8%B3%E5%81%A5"><span class="toc-number">1.2.5.</span> <span class="toc-text">2.5 稳键标准化（robust standardization）：先减去中位数，再除以四分位间距（interquartile range），因为不涉及极值，因此在数据里有异常值的情况下表现比较稳健</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.3.</span> <span class="toc-text">3. 标准化和归一化的区别是什么？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-%E5%BD%92%E4%B8%80%E5%8C%96normalizationnormalization%E5%B0%86%E4%B8%80%E5%88%97%E6%95%B0%E6%8D%AE%E5%8F%98%E5%8C%96%E5%88%B0%E6%9F%90%E4%B8%AA%E5%9B%BA%E5%AE%9A%E5%8C%BA%E9%97%B4%E8%8C%83%E5%9B%B4%E4%B8%AD%E9%80%9A%E5%B8%B8%E8%BF%99%E4%B8%AA%E5%8C%BA%E9%97%B4%E6%98%AF0-1%E5%B9%BF%E4%B9%89%E7%9A%84%E8%AE%B2%E5%8F%AF%E4%BB%A5%E6%98%AF%E5%90%84%E7%A7%8D%E5%8C%BA%E9%97%B4%E6%AF%94%E5%A6%82%E6%98%A0%E5%B0%84%E5%88%B001%E4%B8%80%E6%A0%B7%E5%8F%AF%E4%BB%A5%E7%BB%A7%E7%BB%AD%E6%98%A0%E5%B0%84%E5%88%B0%E5%85%B6%E4%BB%96%E8%8C%83%E5%9B%B4%E5%9B%BE%E5%83%8F%E4%B8%AD%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%98%A0%E5%B0%84%E5%88%B00255%E5%85%B6%E4%BB%96%E6%83%85%E5%86%B5%E5%8F%AF%E8%83%BD%E6%98%A0%E5%B0%84%E5%88%B0-11"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 归一化 (NormalizationNormalization)：将一列数据变化到某个固定区间 (范围) 中，通常，这个区间是 [0, 1]，广义的讲，可以是各种区间，比如映射到 [0，1] 一样可以继续映射到其他范围，图像中可能会映射到 [0,255]，其他情况可能映射到 [-1,1]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-%E6%A0%87%E5%87%86%E5%8C%96standardizationstandardization%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2%E4%B8%BA%E5%9D%87%E5%80%BC%E4%B8%BA0%E6%A0%87%E5%87%86%E5%B7%AE%E4%B8%BA1%E7%9A%84%E5%88%86%E5%B8%83%E5%88%87%E8%AE%B0%E5%B9%B6%E9%9D%9E%E4%B8%80%E5%AE%9A%E6%98%AF%E6%AD%A3%E6%80%81%E7%9A%84"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 标准化 (StandardizationStandardization)：将数据变换为均值为 0，标准差为 1 的分布切记，并非一定是正态的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#33-%E4%B8%AD%E5%BF%83%E5%8C%96%E5%8F%A6%E5%A4%96%E8%BF%98%E6%9C%89%E4%B8%80%E7%A7%8D%E5%A4%84%E7%90%86%E5%8F%AB%E5%81%9A%E4%B8%AD%E5%BF%83%E5%8C%96%E4%B9%9F%E5%8F%AB%E9%9B%B6%E5%9D%87%E5%80%BC%E5%A4%84%E7%90%86%E5%B0%B1%E6%98%AF%E5%B0%86%E6%AF%8F%E4%B8%AA%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E5%87%8F%E5%8E%BB%E8%BF%99%E4%BA%9B%E6%95%B0%E6%8D%AE%E7%9A%84%E5%9D%87%E5%80%BC"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 中心化：另外，还有一种处理叫做中心化，也叫零均值处理，就是将每个原始数据减去这些数据的均值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E7%94%A8%E6%A0%87%E5%87%86%E5%8C%96%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E7%94%A8%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.4.</span> <span class="toc-text">4. 什么时候用标准化，什么时候用归一化？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BB%80%E4%B9%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">1.5.</span> <span class="toc-text">5. 什么机器学习模型需要用到特征缩放？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#51-%E9%80%9A%E8%BF%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%B1%82%E8%A7%A3%E7%9A%84%E6%A8%A1%E5%9E%8B%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E8%BF%99%E5%8C%85%E6%8B%AC%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92linear-regression-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92logistic-regression-%E6%84%9F%E7%9F%A5%E6%9C%BAperceptron-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cneural-network%E7%AD%89%E6%A8%A1%E5%9E%8B-%E6%AD%A4%E5%A4%96%E8%BF%91%E9%82%BB%E6%B3%95knnk%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BBk-means%E7%AD%89%E9%9C%80%E8%A6%81%E6%A0%B9%E6%8D%AE%E6%95%B0%E6%8D%AE%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB%E6%9D%A5%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E7%9A%84%E7%AE%97%E6%B3%95%E4%B9%9F%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90lda%E7%AD%89%E9%9C%80%E8%A6%81%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E7%9A%84%E6%96%B9%E5%B7%AE%E7%9A%84%E7%AE%97%E6%B3%95%E4%B9%9F%E4%BC%9A%E5%8F%97%E5%88%B0%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 通过梯度下降法求解的模型需要进行特征缩放，这包括线性回归（Linear Regression）、逻辑回归（Logistic Regression）、感知机（Perceptron）、支持向量机（SVM）、神经网络（Neural Network）等模型。此外，近邻法（KNN），K 均值聚类（K-Means）等需要根据数据间的距离来划分数据的算法也需要进行特征缩放。主成分分析（PCA），线性判别分析（LDA）等需要计算特征的方差的算法也会受到特征缩放的影响。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#52-%E5%86%B3%E7%AD%96%E6%A0%91decision-tree%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97random-forest%E7%AD%89%E5%9F%BA%E4%BA%8E%E6%A0%91%E7%9A%84%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E4%B8%8D%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E5%9B%A0%E4%B8%BA%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E4%B8%8D%E4%BC%9A%E6%94%B9%E5%8F%98%E6%A0%B7%E6%9C%AC%E5%9C%A8%E7%89%B9%E5%BE%81%E4%B8%8A%E7%9A%84%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 决策树（Decision Tree），随机森林（Random Forest）等基于树的分类模型不需要进行特征缩放，因为特征缩放不会改变样本在特征上的信息增益。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%9C%A8%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E7%9A%84%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9"><span class="toc-number">1.6.</span> <span class="toc-text">6. 在进行特征缩放的时候需要注意的地方？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">1.7.</span> <span class="toc-text">7. 参考链接</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/2022/11/20/github%E6%96%87%E4%BB%B6%E5%A4%B9%E6%9C%89%E7%99%BD%E8%89%B2%E7%AE%AD%E5%A4%B4%E5%B9%B6%E4%B8%94%E4%B8%8D%E8%83%BD%E6%89%93%E5%BC%80%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/" rel="bookmark" title="github文件夹有白色箭头并且不能打开的解决办法">github文件夹有白色箭头并且不能打开的解决办法</a></li><li><a href="/2022/11/23/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B9%8B%E6%89%87%E5%BD%A2%E4%B8%8E%E7%9F%A9%E5%BD%A2%E5%9B%BE%E5%83%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2/" rel="bookmark" title="图像处理之扇形与矩形图像之间的相互转换">图像处理之扇形与矩形图像之间的相互转换</a></li><li><a href="/2022/11/23/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" rel="bookmark" title="激活函数">激活函数</a></li><li><a href="/2022/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E4%BA%8C/" rel="bookmark" title="机器学习实验二">机器学习实验二</a></li><li><a href="/2022/11/23/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/" rel="bookmark" title="维度灾难">维度灾难</a></li><li><a href="/2022/11/23/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="bookmark" title="特征工程">特征工程</a></li><li><a href="/2022/11/23/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/" rel="bookmark" title="交叉验证">交叉验证</a></li><li><a href="/2022/11/23/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E8%A1%A5%E5%85%85/" rel="bookmark" title="交叉验证补充">交叉验证补充</a></li><li><a href="/2022/11/23/Jupyter-Notebook/" rel="bookmark" title="Jupyter Notebook">Jupyter Notebook</a></li><li class="active"><a href="/2022/11/23/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/" rel="bookmark" title="特征缩放">特征缩放</a></li><li><a href="/2022/11/23/L1%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96/" rel="bookmark" title="L1和L2正则化">L1和L2正则化</a></li><li><a href="/2022/11/23/%E5%9C%A8hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/" rel="bookmark" title="在hexo搭建博客遇到的问题">在hexo搭建博客遇到的问题</a></li><li><a href="/2022/11/23/%E6%8F%92%E5%85%A5YouTube%E8%A7%86%E9%A2%91/" rel="bookmark" title="插入YouTube视频">插入YouTube视频</a></li><li><a href="/2022/11/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B4%BE%E7%B3%BB/" rel="bookmark" title="机器学习派系">机器学习派系</a></li><li><a href="/2022/11/24/%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2/" rel="bookmark" title="网格搜索">网格搜索</a></li><li><a href="/2022/11/24/pip%E5%AE%89%E8%A3%85%E6%96%B0%E5%BA%93/" rel="bookmark" title="pip安装新库">pip安装新库</a></li><li><a href="/2022/11/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%E5%AD%A6%E4%B9%A0%E4%B8%80/" rel="bookmark" title="卷积神经网络CNN学习一">卷积神经网络CNN学习一</a></li><li><a href="/2022/11/24/%E7%8C%AB%E7%8B%97%E5%A4%A7%E6%88%98%E4%B9%8BCNN%E5%88%86%E7%B1%BB/" rel="bookmark" title="猫狗大战之CNN分类">猫狗大战之CNN分类</a></li><li><a href="/2022/12/11/docker%E6%89%93%E5%BC%80%E6%8A%A5%E9%94%99/" rel="bookmark" title="docker打开报错">docker打开报错</a></li><li><a href="/2022/12/13/%E5%85%B3%E4%BA%8E%E9%82%A3%E4%BA%9B%E6%88%91%E4%B8%80%E7%9B%B4%E6%90%9E%E4%B8%8D%E6%87%82%E7%9A%84%E5%B8%82%E5%88%B6%E5%8D%95%E4%BD%8D/" rel="bookmark" title="关于那些我一直搞不懂的市制单位">关于那些我一直搞不懂的市制单位</a></li><li><a href="/2023/01/13/TPU%E5%92%8CGPU%E7%9A%84%E4%B8%8D%E5%90%8C%E4%B9%8B%E5%A4%84/" rel="bookmark" title="The difference between GPU AND TPU">The difference between GPU AND TPU</a></li><li><a href="/2023/03/10/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85Git/" rel="bookmark" title="Ubuntu下安装Git">Ubuntu下安装Git</a></li><li><a href="/2023/03/11/%E5%88%86%E6%9E%90%E5%BC%80%E6%BA%90MusicPlayer%E7%9A%84%E4%B8%80%E4%BA%9B%E6%94%B6%E8%8E%B7/" rel="bookmark" title="分析开源MusicPlayer的一些收获">分析开源MusicPlayer的一些收获</a></li><li><a href="/2023/03/12/Qt-class-FAQ/" rel="bookmark" title="Qt class FAQ">Qt class FAQ</a></li><li><a href="/2023/03/12/Ubuntu-practise-C-inward/" rel="bookmark" title="Ubuntu practise C++ inward">Ubuntu practise C++ inward</a></li><li><a href="/2023/03/13/%E7%94%A8Qt-Designer%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%9A%84%E6%AD%A5%E9%AA%A4/" rel="bookmark" title="用Qt-Designer设计一个应用的步骤">用Qt-Designer设计一个应用的步骤</a></li><li><a href="/2023/03/13/Qt-Designer%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/" rel="bookmark" title="Qt-Designer常见操作">Qt-Designer常见操作</a></li><li><a href="/2023/03/22/Qt%E6%8E%A7%E4%BB%B6%E5%AD%A6%E4%B9%A02/" rel="bookmark" title="Qt控件学习2">Qt控件学习2</a></li><li><a href="/2023/03/23/libGL/" rel="bookmark" title="Linux环境下Qt报错libGL">Linux环境下Qt报错libGL</a></li><li><a href="/2023/03/28/Qt%E6%8E%A7%E4%BB%B6%E5%AD%A6%E4%B9%A03/" rel="bookmark" title="Qt控件学习3">Qt控件学习3</a></li><li><a href="/2023/04/05/obsidian%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7-%E5%90%AB%E9%93%BE%E6%8E%A5/" rel="bookmark" title="obsidian双向链接">obsidian双向链接</a></li><li><a href="/2023/04/23/xlog-%E5%9B%BE%E5%BA%8A/" rel="bookmark" title="xlog/图床">xlog/图床</a></li><li><a href="/2023/04/30/Linux-%E4%B8%AD%E7%9A%84-32-%E4%BD%8D%E4%B8%8E-64-%E4%BD%8D%E5%8C%BA%E5%88%AB/" rel="bookmark" title="Linux中32位与64位区别">Linux中32位与64位区别</a></li><li><a href="/2023/05/22/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" rel="bookmark" title="视频理解的基础知识">视频理解的基础知识</a></li><li><a href="/2023/07/20/Linux%E7%B3%BB%E7%BB%9F%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/" rel="bookmark" title="Linux系统目录结构">Linux系统目录结构</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Percy Everard" data-src="/images/magic.png"><p class="name" itemprop="name">Percy Everard</p><div class="description" itemprop="description">Really start to record my life, no longer lazy!</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">69</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">54</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3BlcmN5ZXZlcmFyZA==" title="https:&#x2F;&#x2F;github.com&#x2F;percyeverard"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvbXkvbS9tdXNpYy9wbGF5bGlzdD9pZD0yMjk0OTY3NTIy" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;my&#x2F;m&#x2F;music&#x2F;playlist?id&#x3D;2294967522"><i class="ic i-cloud-music"></i></span> <span class="exturl item email" data-url="bWFpbHRvOmZlbmd5ZXBpYW9zYUAxNjMuY29t" title="mailto:fengyepiaosa@163.com"><i class="ic i-envelope"></i></span> <span class="exturl item instagram" data-url="aHR0cHM6Ly9pbnN0YWdyYW0uY29tL2NoYXJsZXNmZW5n" title="https:&#x2F;&#x2F;instagram.com&#x2F;charlesfeng"><i class="ic i-instagram"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2022/11/23/Jupyter-Notebook/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2022/11/23/L1%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%8C%96/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"></div><div class="status"><div class="copyright">&copy; 2021 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="ic i-wenfeng"></i> </span><span class="author" itemprop="copyrightHolder">Percy Everard @ Percy Feng</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">156k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:21</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2022/11/23/特征缩放/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->